{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the datasets\n",
    "\n",
    "Datasets used in this project:\n",
    "\n",
    "* [Ryerson Audio-Visual Database of Emotional Speech and Song (Ravdess)](#rav)\n",
    "* [Crowd-sourced Emotional Mutimodal Actors Dataset (Crema-D)](#crema)\n",
    "* [Surrey Audio-Visual Expressed Emotion (Savee)](#save)\n",
    "* [Toronto emotional speech set (Tess)](#tess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array operations and useful analysis functionalities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# provide tools to deal with filenames, paths, directories\n",
    "import os\n",
    "import sys\n",
    " \n",
    "# Libraries for Audio files\n",
    "import librosa\n",
    "import librosa.display\n",
    "import glob\n",
    "\n",
    "# To play sound in the notebook\n",
    "import IPython.display as ipd \n",
    "\n",
    "# Libraries for visualisations in the notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEE = \"./data/Savee/ALL/\"\n",
    "TESS = \"./data/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
    "CREMA = \"./data/Crema/AudioWAV/\"\n",
    "RAV = \"./data/RAVDESS/audio_speech_actors_01-24/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring RAVDESS dataset <a class=\"anchor\" id=\"rav\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionaries and function to decipher the information in the files\n",
    "modality = {'01':'full_av','02':'video_only','03':'audio_only'}\n",
    "vocal_channel = {'01':'speech','02':'song'}\n",
    "emotion = {'01':'neutral','02':'calm','03':'happy','04':'sad','05':'angry','06':'fear','07':'disgust','08':'surprised'}\n",
    "emotional_intensity = {'01':'normal','02':'strong'}\n",
    "statement = {'01':'Kids are talking by the door','02':'Dogs are sitting by the door'}\n",
    "reptition = {'01':'first_repitition','02':'second_repetition'}\n",
    "def actor_f(num):\n",
    "    if int(num)%2==0: return('female')\n",
    "    else: return('male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/RAVDESS/audio_speech_actors_01-24/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1321bec91d9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# List of the actors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrav_directory_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/RAVDESS/audio_speech_actors_01-24/'"
     ]
    }
   ],
   "source": [
    "# List of the actors \n",
    "rav_directory_list = sorted(os.listdir(RAV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the actors, we obtain the label information from the filenames:\n",
    "rav_audio_dict = {}\n",
    "for actor in rav_directory_list:\n",
    "    actor_dir = os.path.join(RAV,actor)\n",
    "    actor_files = os.listdir(actor_dir)\n",
    "    actor_dict = [i.replace(\".wav\",\"\").split(\"-\") for i in actor_files]\n",
    "    dict_entry = {os.path.join(actor_dir,i):j for i,j in zip(actor_files,actor_dict)}\n",
    "    rav_audio_dict.update(dict_entry)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cast the in information  as a dataframe\n",
    "#transpose it so that the labels appear in the columns and the filenames become the row indices\n",
    "rav_audio_dict = pd.DataFrame(rav_audio_dict).T\n",
    "\n",
    "# give the columns an appropriate name\n",
    "rav_audio_dict.columns = ['modality','vocal_channel','emotion','emotional_intensity','statement','repetition','actor']\n",
    "rav_audio_dict.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the dictionaries and function created above to transform the digit-encoded labels into human-readable format:\n",
    "rav_audio_dict.modality = rav_audio_dict.modality.map(modality)\n",
    "rav_audio_dict.vocal_channel = rav_audio_dict.vocal_channel.map(vocal_channel)\n",
    "rav_audio_dict.emotion = rav_audio_dict.emotion.map(emotion)\n",
    "rav_audio_dict.emotional_intensity = rav_audio_dict.emotional_intensity.map(emotional_intensity)\n",
    "rav_audio_dict.statement = rav_audio_dict.statement.map(statement)\n",
    "rav_audio_dict.repetition = rav_audio_dict.repetition.map(reptition)\n",
    "rav_audio_dict['actor_gender'] = rav_audio_dict.actor.apply(actor_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rav_audio_dict.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(2, 2,figsize=(12,8))\n",
    "\n",
    "ax1[0].barh(y=rav_audio_dict.emotion.value_counts().index,width=rav_audio_dict.emotion.value_counts().values, color = '#1ac7c8')\n",
    "ax1[0].set_title('Emotion')\n",
    "\n",
    "ax1[1].bar(x=rav_audio_dict.actor_gender.value_counts().index,height=rav_audio_dict.actor_gender.value_counts().values)\n",
    "ax1[1].set_title('Actor Gender')\n",
    "\n",
    "ax2[0].bar(x=rav_audio_dict.emotional_intensity.value_counts().index,height=rav_audio_dict.emotional_intensity.value_counts().values)\n",
    "ax2[0].set_title('Emotional Intensity')\n",
    "\n",
    "ax2[1].bar(x=rav_audio_dict.statement.value_counts().index,height=rav_audio_dict.statement.value_counts().values)\n",
    "plt.xticks(rotation=45)\n",
    "ax2[1].set_title('Statement')\n",
    "\n",
    "# Remove empty white space around the plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a happy track (female)\n",
    "fname_happy = RAV + 'Actor_22/03-01-03-01-02-01-22.wav'\n",
    "# sad (female)\n",
    "fname_sad = RAV + 'Actor_17/03-01-04-02-01-02-17.wav' \n",
    "# 2. Load the audio as a waveform `y`\n",
    "#    Store the sampling rate as `sr`\n",
    "y,sr = librosa.load(fname_happy)\n",
    "y2,sr = librosa.load(fname_sad)\n",
    "fig, ax= plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\n",
    "librosa.display.waveplot(y, sr=sr, ax =ax [0],color='#1ac7c8')\n",
    "ax[0].set(title='Happy (f)')\n",
    "\n",
    "librosa.display.waveplot(y2, sr=sr, ax =ax [1])\n",
    "ax[1].set(title='Sad (f)')\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname_happy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring CREMA-D dataset <a class=\"anchor\" id=\"crema\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Crema/AudioWAV/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d709ac7a85ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrema_directory_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCREMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Crema/AudioWAV/'"
     ]
    }
   ],
   "source": [
    "crema_directory_list = sorted(os.listdir(CREMA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each actor obtain the label information from the filenames\n",
    "crema_audio_dict = {}\n",
    "for i in crema_directory_list:\n",
    "    actor_dir = os.path.join(CREMA,i)\n",
    "    actor_dict = i.replace(\".wav\",\"\").split(\"_\")\n",
    "    dict_entry = {os.path.join(actor_dir):actor_dict}\n",
    "    crema_audio_dict.update(dict_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crema_audio_dict = pd.DataFrame(crema_audio_dict).T\n",
    "crema_audio_dict.columns = ['actor','statement','emotion','emotional_intensity']\n",
    "crema_audio_dict.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionaries and function to  decipher the information in the files\n",
    "emotion = {'ANG':'angry','DIS':'disgust','FEA':'fear','HAP':'happy','NEU':'neutral','SAD':'sad'}\n",
    "emotional_intensity = {'LO':'low','MD':'medium','HI':'high','XX':'unspecified'}\n",
    "statement = {'IEO':'Its eleven oclock' ,'TIE':'That is exactly what happened', 'IOM': 'I''m on my way to the meeting',\n",
    "             'IWW':'I wonder what this is about','TAI':'The airplane is almost full','MTI':'Maybe tomorrow it will be cold',\n",
    "             'IWL':'I would like a new alarm clock','ITH':'I think I have a doctor''s appointment','DFA':'Don''t forget a jacket.',\n",
    "             'ITS':'I think I''ve seen this before','TSI':'The surface is slick','WSI':'We''ll stop in a couple of minutes'}\n",
    "#female voices \n",
    "female = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n",
    "          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n",
    "\n",
    "def actor_f(num):\n",
    "    if int(num) in female:return('female')\n",
    "    else: return('male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the digits to a human-readable text\n",
    "crema_audio_dict['actor_gender'] = crema_audio_dict.actor.apply(actor_f)\n",
    "crema_audio_dict.statement = crema_audio_dict.statement.map(statement)\n",
    "crema_audio_dict.emotion = crema_audio_dict.emotion.map(emotion)\n",
    "crema_audio_dict.emotional_intensity = crema_audio_dict.emotional_intensity.map(emotional_intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crema_audio_dict.head (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(2, 2,figsize=(12,8))\n",
    "\n",
    "ax1[0].barh(y=crema_audio_dict.emotion.value_counts().index,width=crema_audio_dict.emotion.value_counts().values, color = '#1ac7c8')\n",
    "ax1[0].set_title('Emotion')\n",
    "\n",
    "ax1[1].bar(x=crema_audio_dict.actor_gender.value_counts().index,height=crema_audio_dict.actor_gender.value_counts().values)\n",
    "ax1[1].set_title('Actor Gender')\n",
    "\n",
    "ax2[0].bar(x=crema_audio_dict.emotional_intensity.value_counts().index,height=crema_audio_dict.emotional_intensity.value_counts().values)\n",
    "ax2[0].set_title('Emotional Intensity')\n",
    "\n",
    "ax2[1].bar(x=crema_audio_dict.statement.value_counts().index,height=crema_audio_dict.statement.value_counts().values)\n",
    "ax2[1].set_title('Statement')\n",
    "# 'ha'changes the alignment to right-aligned \n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Remove empty white space around the plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring SAVEE dataset  <a class=\"anchor\" id=\"save\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savee_directory_list = sorted(os.listdir(SAVEE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savee_audio_dict = {}\n",
    "for i in savee_directory_list:\n",
    "    actor_dir = os.path.join(SAVEE,i)\n",
    "    actor_dict = i.replace(\".wav\",\"\").split(\"_\")\n",
    "    # the last two character from the 2nd sting show the number of the sentence\n",
    "    # remove them since they are not relevant\n",
    "    second_elements = actor_dict[1]\n",
    "    for x in second_elements:\n",
    "        actor_dict[1]=second_elements[:-2]\n",
    "        \n",
    "    dict_entry = {os.path.join(actor_dir):actor_dict}\n",
    "    savee_audio_dict.update(dict_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "savee_audio_dict = pd.DataFrame(savee_audio_dict).T\n",
    "savee_audio_dict.columns = ['actor','emotion']\n",
    "savee_audio_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary to  decipher the information in the files\n",
    "emotion = {'a':'angry','d':'disgust','f':'fear','su':'surprised','h':'happy','n':'neutral','sa':'sad'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# map the digits to a human-readable text\n",
    "savee_audio_dict.emotion = savee_audio_dict.emotion.map(emotion)\n",
    "# this dataset contains only male audio files \n",
    "savee_audio_dict['actor_gender'] = \"male\"\n",
    "savee_audio_dict.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n",
    "axes[0].barh(y=savee_audio_dict.emotion.value_counts().index,width=savee_audio_dict.emotion.value_counts().values, color = '#1ac7c8')\n",
    "axes[0].set_title('Emotion')\n",
    "\n",
    "axes[1].bar(x=savee_audio_dict.actor_gender.value_counts().index,height=savee_audio_dict.actor_gender.value_counts().values)\n",
    "axes[1].set_title('Actor Gender')\n",
    "\n",
    "# Remove empty white space around the plot\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring TESS dataset  <a class=\"anchor\" id=\"tess\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_directory_list = sorted(os.listdir(TESS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_audio_dict = {}\n",
    "for actor in tess_directory_list:\n",
    "    actor_dir = os.path.join(TESS,actor)\n",
    "    actor_files = os.listdir(actor_dir)\n",
    "    actor_dict = [i.replace(\".wav\",\"\").split(\"_\") for i in actor_files]\n",
    "    dict_entry = {os.path.join(actor_dir,i):j for i,j in zip(actor_files,actor_dict)}\n",
    "    tess_audio_dict.update(dict_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tess_audio_dict = pd.DataFrame(tess_audio_dict).T\n",
    "tess_audio_dict.columns = ['actor','statement','emotion']\n",
    "tess_audio_dict['actor_gender'] = \"female\"\n",
    "tess_audio_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary to  decipher the information in the files\n",
    "# plesent surprise will be labeled as suprised\n",
    "emotion = {'angry':'angry','disgust':'disgust','fear':'fear',\n",
    "           'ps':'surprised','happy':'happy','neutral':'neutral','sad':'sad'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_audio_dict.emotion = tess_audio_dict.emotion.map(emotion)\n",
    "tess_audio_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n",
    "axes[0].barh(y=tess_audio_dict.emotion.value_counts().index,width=tess_audio_dict.emotion.value_counts().values, color = '#1ac7c8')\n",
    "axes[0].set_title('Emotion')\n",
    "\n",
    "axes[1].bar(x=tess_audio_dict.actor_gender.value_counts().index,height=tess_audio_dict.actor_gender.value_counts().values)\n",
    "axes[1].set_title('Actor Gender')\n",
    "\n",
    "# Remove empty white space around the plot\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dataframe using the data frames created so far "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_df=rav_audio_dict[[\"emotion\",\"actor_gender\"]]\n",
    "CREMA_df=crema_audio_dict[[\"emotion\",\"actor_gender\"]]\n",
    "\n",
    "SAVE_df=savee_audio_dict[[\"emotion\",\"actor_gender\"]]\n",
    "\n",
    "TESS_df=tess_audio_dict[[\"emotion\",\"actor_gender\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rav_audio_dict[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a dataframe \n",
    "df = pd.concat([RAV_df,CREMA_df,SAVE_df,TESS_df], axis = 0)\n",
    "print(df.emotion.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['emotion'])\n",
    "df.index.name='path'\n",
    "df.to_csv(\"data_path.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.index.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.xticks(fontsize=15)\n",
    "sns.countplot('emotion', data=df,palette=\"mako\") # mako\n",
    "#sns.countplot('emotion', data=df,palette=\"Set3_r\")\n",
    "plt.title('Count of Emotions', size=20)\n",
    "plt.ylabel('Count', size=15)\n",
    "plt.xlabel('Emotions', size=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.xticks(fontsize=15)\n",
    "\n",
    "sns.countplot(x = 'emotion', hue = \"actor_gender\", data = df, palette=\"mako\")\n",
    "plt.ylabel('Count', size=15)\n",
    "plt.xlabel('Emotions', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost every dataset has an unequal representation of classes. This isn’t a problem as long as the difference is small. However, in this case some classes contain many more examples than the other (calm and surprised). This is a problem since many models don’t work very well at identifying the minority classes. The classifiers tend to ignore small classes while concentrating on classifying the large ones accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_female = df[df.actor_gender == 'female']\n",
    "df_female.to_csv(\"data_female_path.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_male=df[df.actor_gender == 'male']\n",
    "df_male.to_csv(\"data_male_path.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the male neutral emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add paths and get signals.\n",
    "file1= RAV + 'Actor_21/03-01-01-01-02-02-21.wav'\n",
    "file2= CREMA + '1048_DFA_NEU_XX.wav'\n",
    "file3=SAVEE + 'JK_n04.wav'\n",
    "file4= TESS + 'YAF_neutral/YAF_soup_neutral.wav'\n",
    "\n",
    "# Load the audio as a waveform `file(n)` and store the sampling rate as `sr`\n",
    "signal1,sample_rate = librosa.load(file1, sr=22050)\n",
    "signal2,sample_rate = librosa.load(file2, sr=22050)\n",
    "signal3,sample_rate = librosa.load(file3, sr=22050)\n",
    "signal4,sample_rate = librosa.load(file4, sr=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "emotion = 'Neutral'\n",
    "\n",
    "# WAVEFORM\n",
    "# display waveform\n",
    "plt.subplot(2, 2, 1)\n",
    "librosa.display.waveplot(signal1,sample_rate, alpha=0.4)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"RAVDESS Waveform \"+emotion)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "librosa.display.waveplot(signal2,sample_rate, alpha=0.4)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"CREMA-D Waveform \"+emotion)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.subplot(2, 2, 3)\n",
    "librosa.display.waveplot(signal3,sample_rate, alpha=0.4)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"SAVEE Waveform \"+emotion)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "librosa.display.waveplot(signal4,sample_rate, alpha=0.4)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"TESS Waveform \"+emotion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aplitude difference is quite visible, which is probably due to different recording environments, text and individual voice characterstics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing Fourier transformations to draw the power spectrums\n",
    "# FFT -> power spectrum\n",
    "# perform Fourier transform\n",
    "fft1 = np.fft.fft(signal1)\n",
    "fft2 = np.fft.fft(signal2)\n",
    "fft3 = np.fft.fft(signal3)\n",
    "fft4 = np.fft.fft(signal4)\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrum1 = np.abs(fft1)\n",
    "spectrum2 = np.abs(fft2)\n",
    "spectrum3 = np.abs(fft3)\n",
    "spectrum4 = np.abs(fft4)\n",
    "\n",
    "# create frequency variable\n",
    "f1 = np.linspace(0, sample_rate, len(spectrum1))\n",
    "f2 = np.linspace(0, sample_rate, len(spectrum2))\n",
    "f3 = np.linspace(0, sample_rate, len(spectrum3))\n",
    "f4 = np.linspace(0, sample_rate, len(spectrum4))\n",
    "\n",
    "# take half of the spectrum and frequency\n",
    "left_spectrum1 = spectrum1[:int(len(spectrum1)/2)]\n",
    "left_f1 = f1[:int(len(spectrum1)/2)]\n",
    "# take half of the spectrum and frequency\n",
    "left_spectrum2 = spectrum2[:int(len(spectrum2)/2)]\n",
    "left_f2 = f2[:int(len(spectrum2)/2)]\n",
    "# take half of the spectrum and frequency\n",
    "left_spectrum3 = spectrum3[:int(len(spectrum3)/2)]\n",
    "left_f3 = f3[:int(len(spectrum3)/2)]\n",
    "# take half of the spectrum and frequency\n",
    "left_spectrum4 = spectrum4[:int(len(spectrum4)/2)]\n",
    "left_f4 = f4[:int(len(spectrum4)/2)]\n",
    "\n",
    "fig = plt.figure(figsize=(8,10))\n",
    "plt.subplot(2, 2, 1)\n",
    "# plot spectrum\n",
    "plt.plot(left_f1, left_spectrum1, alpha=0.4)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.title(\"RAVDESS  Power spectrum \"+emotion)\n",
    "\n",
    "plt.subplot(2, 2,2)\n",
    "# plot spectrum\n",
    "plt.plot(left_f2, left_spectrum2, alpha=0.4)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.title(\"CREMA-D Power spectrum \"+emotion)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,10))\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(left_f3, left_spectrum3, alpha=0.4)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.title(\"SAVEE Power spectrum \"+emotion)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(left_f4, left_spectrum4, alpha=0.4)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.title(\"TESS Power spectrum \"+emotion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power spectrums between the datasets are also very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT -> spectrogram\n",
    "hop_length =256 # in num. of samples\n",
    "n_fft = 4096 # window in num. of samples\n",
    "\n",
    "# calculate duration hop length and window in seconds\n",
    "hop_length_duration = float(hop_length)/sample_rate\n",
    "n_fft_duration = float(n_fft)/sample_rate\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
    "\n",
    "# perform stft\n",
    "stft1 = librosa.stft(signal1, n_fft=n_fft, hop_length=hop_length)\n",
    "stft2 = librosa.stft(signal2, n_fft=n_fft, hop_length=hop_length)\n",
    "stft3 = librosa.stft(signal3, n_fft=n_fft, hop_length=hop_length)\n",
    "stft4 = librosa.stft(signal4, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrogram1 = np.abs(stft1)\n",
    "spectrogram2 = np.abs(stft2)\n",
    "spectrogram3 = np.abs(stft3)\n",
    "spectrogram4 = np.abs(stft4)\n",
    "\n",
    "\n",
    "# display spectrogram\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.subplot(2, 2, 1)\n",
    "librosa.display.specshow(spectrogram1, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "plt.title(\"RAVDESS Spectrogram \"+emotion)\n",
    "\n",
    "plt.subplot(2, 2,2)\n",
    "librosa.display.specshow(spectrogram2, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "plt.title(\"CREMA-D Spectrogram \"+emotion)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.subplot(2, 2, 3)\n",
    "librosa.display.specshow(spectrogram3, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "plt.title(\"SAVEE Spectrogram \"+emotion)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "librosa.display.specshow(spectrogram4, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "plt.title(\"TESS Spectrogram \"+emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast amplitude to decibels\n",
    "log_spectrogram1 = librosa.amplitude_to_db(spectrogram1)\n",
    "log_spectrogram2 = librosa.amplitude_to_db(spectrogram2)\n",
    "log_spectrogram3 = librosa.amplitude_to_db(spectrogram3)\n",
    "log_spectrogram4 = librosa.amplitude_to_db(spectrogram4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.subplot(2, 2, 1)\n",
    "librosa.display.specshow(log_spectrogram1, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"RAVDESS Spectogramm (dB) \"+emotion)\n",
    "\n",
    "plt.subplot(2, 2,2)\n",
    "librosa.display.specshow(log_spectrogram2, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"CREMA-D Spectogramm (dB) \"+emotion)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.subplot(2, 2, 3)\n",
    "librosa.display.specshow(log_spectrogram3, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"SAVEE Spectogramm (dB) \"+emotion)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "librosa.display.specshow(log_spectrogram4, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"TESS Spectogramm (dB) \"+emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "# extract 13 MFCCs\n",
    "MFCCs1 = librosa.feature.mfcc(signal1, sample_rate, n_fft=4096, hop_length=256, n_mfcc=40)\n",
    "MFCCs2 = librosa.feature.mfcc(signal2, sample_rate, n_fft=4096, hop_length=256, n_mfcc=40)\n",
    "MFCCs3 = librosa.feature.mfcc(signal3, sample_rate, n_fft=4096, hop_length=256, n_mfcc=40)\n",
    "MFCCs4 = librosa.feature.mfcc(signal4, sample_rate, n_fft=4096, hop_length=256, n_mfcc=40)\n",
    "\n",
    "# display MFCCs\n",
    "hop_length=256\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.subplot(2, 2, 1)\n",
    "librosa.display.specshow(MFCCs1, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "plt.title(\"RAVDESS MFCCs \"+emotion)\n",
    "\n",
    "plt.subplot(2, 2,2)\n",
    "librosa.display.specshow(MFCCs2, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "plt.title(\"CREMA-D MFCCs \"+emotion)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.subplot(2, 2, 3)\n",
    "librosa.display.specshow(MFCCs3, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "plt.title(\"SAVEE MFCCs \"+emotion)\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "librosa.display.specshow(MFCCs4, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "plt.title(\"TESS MFCCs \"+emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
